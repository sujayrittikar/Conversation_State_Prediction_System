{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RFO4yV000Tzo",
        "outputId": "3b21279a-e97d-403c-bff4-3e57041464d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting resemblyzer\n",
            "  Downloading Resemblyzer-0.1.1.dev0-py3-none-any.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 625 kB/s \n",
            "\u001b[?25hCollecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting webrtcvad>=2.0.10\n",
            "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.7/dist-packages (from resemblyzer) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from resemblyzer) (1.11.0+cu113)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from resemblyzer) (1.4.1)\n",
            "Requirement already satisfied: librosa>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from resemblyzer) (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.1->resemblyzer) (21.3)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.1->resemblyzer) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.1->resemblyzer) (0.10.3.post1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.1->resemblyzer) (1.6.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.1->resemblyzer) (2.1.9)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.1->resemblyzer) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.1->resemblyzer) (0.51.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.1->resemblyzer) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.1->resemblyzer) (1.0.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.1->resemblyzer) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.1->resemblyzer) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa>=0.6.1->resemblyzer) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.1->resemblyzer) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.1->resemblyzer) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.1->resemblyzer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.1->resemblyzer) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.1->resemblyzer) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.1->resemblyzer) (2022.5.18.1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa>=0.6.1->resemblyzer) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.1->resemblyzer) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.6.1->resemblyzer) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.1->resemblyzer) (2.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->resemblyzer) (4.2.0)\n",
            "Building wheels for collected packages: webrtcvad, typing\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp37-cp37m-linux_x86_64.whl size=72383 sha256=4cc25e0dbc3878848b626186fbe3eee324bab441eb5863b88415e9febb51fea1\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/f9/67/a3158d131f57e1c0a7d8d966a707d4a2fb27567a4fe47723ad\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26325 sha256=51ffe20914309f6a872ce5869fd9c5ceeb44b93e98fb81d4e31875506f317d65\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n",
            "Successfully built webrtcvad typing\n",
            "Installing collected packages: webrtcvad, typing, resemblyzer\n",
            "Successfully installed resemblyzer-0.1.1.dev0 typing-3.7.4.3 webrtcvad-2.0.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spectralcluster\n",
            "  Downloading spectralcluster-0.2.5-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: spectralcluster\n",
            "Successfully installed spectralcluster-0.2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install resemblyzer\n",
        "!pip install pydub\n",
        "!pip install spectralcluster\n",
        "# !pip install links_cluster\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import links_cluster"
      ],
      "metadata": {
        "id": "vGyP-Vj-wHsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k264aU8Xm9lg",
        "outputId": "ab5e7a9f-c1b9-444f-b031-9ea4451cc14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: audioread in /usr/local/lib/python3.7/dist-packages (2.1.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub\n",
        "!pip install audioread\n",
        "!pip install SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2qRoW_LnRPs"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ok9mdS2nGZi"
      },
      "outputs": [],
      "source": [
        "# importing libraries \n",
        "import speech_recognition as sr \n",
        "import os \n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "\n",
        "# create a speech recognition object\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# a function that splits the audio file into chunks\n",
        "# and applies speech recognition\n",
        "def get_large_audio_transcription(path):\n",
        "    \"\"\"\n",
        "    Splitting the large audio file into chunks\n",
        "    and apply speech recognition on each of these chunks\n",
        "    \"\"\"\n",
        "    # open the audio file using pydub\n",
        "    sound = AudioSegment.from_wav(path)  \n",
        "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
        "    chunks = split_on_silence(sound,\n",
        "        # experiment with this value for your target audio file\n",
        "        min_silence_len = 500,\n",
        "        # adjust this per requirement\n",
        "        silence_thresh = sound.dBFS-14,\n",
        "        # keep the silence for 1 second, adjustable as well\n",
        "        keep_silence=500,\n",
        "    )\n",
        "    folder_name = \"audio-chunks\"\n",
        "    # create a directory to store the audio chunks\n",
        "    if not os.path.isdir(folder_name):\n",
        "        os.mkdir(folder_name)\n",
        "    whole_text = \"\"\n",
        "    chunksList = []\n",
        "    # process each chunk \n",
        "    for i, audio_chunk in enumerate(chunks, start=1):\n",
        "        # export audio chunk and save it in\n",
        "        # the `folder_name` directory.\n",
        "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
        "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
        "        # recognize the chunk\n",
        "        with sr.AudioFile(chunk_filename) as source:\n",
        "            audio_listened = r.record(source)\n",
        "            # try converting it to text\n",
        "            try:\n",
        "                text = r.recognize_google(audio_listened)\n",
        "            except sr.UnknownValueError as e:\n",
        "                # print(\"Error:\", str(e))\n",
        "                pass\n",
        "            else:\n",
        "                text = f\"{text.capitalize()}. \"\n",
        "                # print(chunk_filename, \":\", text)\n",
        "                chunksList.append(text)\n",
        "                whole_text += text\n",
        "    # return the text for all chunks detected\n",
        "    return (whole_text, chunksList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYEqmhCMNZUg"
      },
      "outputs": [],
      "source": [
        "def byte_to_float(byte):\n",
        "    # byte -> int16(PCM_16) -> float32\n",
        "    return pcm2float(np.frombuffer(byte,dtype=np.int8), dtype='float32')\n",
        "\n",
        "def pcm2float(sig, dtype='float32'):\n",
        "    \"\"\"Convert PCM signal to floating point with a range from -1 to 1.\n",
        "    Use dtype='float32' for single precision.\n",
        "    Parameters\n",
        "    ----------\n",
        "    sig : array_like\n",
        "        Input array, must have integral type.\n",
        "    dtype : data type, optional\n",
        "        Desired (floating point) data type.\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "        Normalized floating point data.\n",
        "    See Also\n",
        "    --------\n",
        "    float2pcm, dtype\n",
        "    \"\"\"\n",
        "    sig = np.asarray(sig)\n",
        "    if sig.dtype.kind not in 'iu':\n",
        "        raise TypeError(\"'sig' must be an array of integers\")\n",
        "    dtype = np.dtype(dtype)\n",
        "    if dtype.kind != 'f':\n",
        "        raise TypeError(\"'dtype' must be a floating point type\")\n",
        "\n",
        "    i = np.iinfo(sig.dtype)\n",
        "    abs_max = 2 ** (i.bits - 1)\n",
        "    offset = i.min + abs_max\n",
        "    return (sig.astype(dtype) - offset) / abs_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILhlzr58ylpM"
      },
      "outputs": [],
      "source": [
        "# all imports\n",
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "from io import BytesIO\n",
        "!pip -q install pydub\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import contextlib\n",
        "import librosa\n",
        "import struct\n",
        "import soundfile\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "def record(sec=10):\n",
        "  display(Javascript(RECORD))\n",
        "  s = output.eval_js('record(%d)' % (sec*1000))\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  # audio = AudioSegment.from_file(BytesIO(b))\n",
        "  # print(BytesIO(b).getvalue())\n",
        "  someByte = bytearray()\n",
        "  someByte.extend(BytesIO(b).getvalue())\n",
        "  # data.extend(BytesIO(b).getvalue())\n",
        "  # print(data)\n",
        "  # byteData = b''.join(data)\n",
        "  # print(byteData)\n",
        "  with open('audioLive.wav','wb') as f:\n",
        "    f.write(bytes(BytesIO(b).getvalue()))\n",
        "  return (someByte, bytes(someByte))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pmmkVD38b3S"
      },
      "outputs": [],
      "source": [
        "from resemblyzer import preprocess_wav, VoiceEncoder\n",
        "from pathlib import Path\n",
        "from pydub.utils import mediainfo\n",
        "\n",
        "\"\"\"\n",
        "Embeddings class has following abilities:\n",
        "1. Find the Speech Segments with existence of voice\n",
        "2. Obtain the d-vector embeddings for each segment\n",
        "3. Perform Clustering over the obtained d-vector embeddings for the speech segments\n",
        "4. Find the speaker labels with start and stop time from the audio file.\n",
        "\n",
        "The class Embeddings has the following parameters being calculated for an audio file while instantiating:\n",
        "1. Sampling rate (sampling_rate)\n",
        "2. Wave Splits of the Speech Segments (wav_splits)\n",
        "3. D-vector embeddings (cont_embeds)\n",
        "\n",
        "Functions:\n",
        "1. Constructor:\n",
        "def __init__(self, audio_file_path) -> Specify the path of audio file to create an Embeddings object for a single file.\n",
        "2. Voice Activity Detection:\n",
        "def VoiceActivityDetection(self) -> Returns the Wave splits of speech segments (wav_splits)\n",
        "3. Embeddings Extraction (d-vectors):\n",
        "def ExtractEmbeddings(self) -> Returns the d-vector embeddings for the speech segments\n",
        "4. Clustering:\n",
        "def OfflineSpectralClustering(self) -> Returns labels, i.e., the speaker identities of the audio file\n",
        "5. Labelings of Speakers:\n",
        "def SpeakerLabeling(self, label) -> Returns a tuple of (label, start time, stop time) for each segment obtained by VoiceActivityDetection (through wav splits)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Embeddings:\n",
        "  def __init__(self, audio_file_path, data_flag, Data=None):\n",
        "    #Technical info (title,date,duration,sample rate,channels,language,etc) of file is stored in variable info using open source mediainfo library\n",
        "    info = mediainfo(audio_file_path)\n",
        "    #Sample rate column from info table is stored in sampling rate\n",
        "    self.sampling_rate = info['sample_rate']\n",
        "    self.audio_file_path = audio_file_path\n",
        "  \n",
        "  #    Here we are using open source library called resemblyzer which is used for 2 tasks: \n",
        "  #    1. Speech Detection and Segmentation\n",
        "  #    2. Embeddings Extraction\n",
        "  #    To instantiate Embeddings object, pass the parameter audio_file_path.\n",
        "    self.data = Data\n",
        "    #Path name of the .wav file is retrieved and stored in wav_fpath\n",
        "    wav_fpath = Path(audio_file_path)\n",
        "\n",
        "  #    Used preprocess_wav() which internally uses a VAD to trim out the silences in the audio file. \n",
        "  #    It is also used to remove outliers present in our speech data and also to normalize the decibel level of audio. \n",
        "  #    Function will take input as our file path and output of that function will give us the processed Audio file.   \n",
        "    if data_flag==0:\n",
        "      wav = preprocess_wav(Data)\n",
        "    else:\n",
        "      wav = preprocess_wav(wav_fpath)\n",
        "\n",
        "  #   Then we create an instance of the VoiceEncoder class as encoder, by passing cpu as default device. \n",
        "  #   VoiceEncoder is deep learning model which is used to derive high-level representation of voice.  \n",
        "    self.encoder = VoiceEncoder(\"cpu\")\n",
        "\n",
        "  #   There is embed_utterance function availabel. This function takes the processed wav file. \n",
        "  #   It will create segments of voice data as wav_splits and as well makes MFCCs of these segments.\n",
        "  #   Finally creates embeds of these audio segments. And stored in cont_embeds. \n",
        "    _, cont_embeds, wav_splits = self.encoder.embed_utterance(wav, return_partials=True, rate= 4)\n",
        "\n",
        "    #start and end time of each window for which a d-vector has been created is stored in wav_splits\n",
        "    self.wav_splits = wav_splits\n",
        "\n",
        "  # The cont_embeds is a N by D matrix, where N is the number of segments created (which is equal to the number of d-vectors) \n",
        "  #  and D is the dimension of each d-vector, which by default is 256. \n",
        "  #  wav_splits is a list with the start and end time of each window for which a d-vector has been created.    \n",
        "    self.cont_embeds = cont_embeds\n",
        "\n",
        "  # In Voice Activity Detection the presence or absence of speech in an audio segment is detected.\n",
        "  #    1. It is done by taking the audio segments which are split into segments of 5-40ms duration as an input.\n",
        "  #    2. Then two features namely signal energy and spectral centroid are extracted and compared to the threshold value.\n",
        "  #    3. Then depending upon the value, the decision about whether or not speech is present is taken.\n",
        "  #    4. The wav_splits containing only speech part are returned\n",
        "  def VoiceActivityDetection(self):\n",
        "    return self.wav_splits\n",
        "\n",
        " # this function will return the embeddings generated while instantiating.\n",
        "  def ExtractEmbeddings(self):\n",
        "    return self.cont_embeds\n",
        "  \n",
        "  #  This function is used to perform spectral offline clustering on our embeddings. we use open source module called SpectralCluster.\n",
        "  #  We can build our own Spectral clustering model using SpectralCluster class. we can set different parameters such as minimum cluster and \n",
        "  #  maximum cluster for our model. In our case we set minimum of 3 and maximum of 100 clusters that our model will produce.\n",
        "  def OfflineSpectralClustering(self, clusterer, vector):\n",
        "    from links_cluster import LinksCluster\n",
        "    predicted_cluster = clusterer.predict(vector)\n",
        "    return predicted_cluster\n",
        "\n",
        "  def SpeakerLabeling(self, labels):\n",
        "    # the number of samples per second is given by sampling_rate\n",
        "    currentLabel = -1\n",
        "    aggregatedLabels = []\n",
        "    for i in range(len(labels)):\n",
        "      if currentLabel!=labels[i]:\n",
        "        aggregatedLabels.append(labels[i])\n",
        "        currentLabel = labels[i]\n",
        "      else:\n",
        "        continue\n",
        "    return aggregatedLabels\n",
        "\n",
        "  def update(self, data):\n",
        "    self.data = data\n",
        "    wav = preprocess_wav(self.data)\n",
        "    _, cont_embeds, wav_splits = self.encoder.embed_utterance(wav, return_partials=True, rate= 4)\n",
        "    self.wav_splits = wav_splits\n",
        "    self.cont_embeds = cont_embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIHk83NzDU1I"
      },
      "outputs": [],
      "source": [
        "# This function is the step by step implementation of all the processes which we have defined in Embeddings Class above. \n",
        "\n",
        "def SpeakerDiarization(embedding_obj, clusterer):\n",
        "   # Create the embedding_obj object of Embedding class.\n",
        "   # Returns the speech segments.\n",
        "  # detectedVoiceSegments = embedding_obj.VoiceActivityDetection()\n",
        "   # Obtain embeddings by calling the function ExtractEmbeddings().\n",
        "  embeddings = embedding_obj.ExtractEmbeddings()\n",
        "   # Apply spectral offline clustering to embeddings to get speaker labels using OfflineSpectralClustering() function.\n",
        "  clusteredLabels = []\n",
        "  print(embeddings.shape)\n",
        "  for i in range(len(embeddings)):\n",
        "    clusteredLabels.append(embedding_obj.OfflineSpectralClustering(clusterer, embeddings[i]))\n",
        "  labels = embedding_obj.SpeakerLabeling(clusteredLabels)\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smdEGu6jMMpg"
      },
      "outputs": [],
      "source": [
        "def getMappedTranscript(currentStatesList, chunksList):\n",
        "  transcriptMap = []\n",
        "  if len(currentStatesList)>0 and len(chunksList)>0:\n",
        "    iteration = max(len(currentStatesList), len(chunksList))\n",
        "    i = 0\n",
        "    temp = currentStatesList[0]\n",
        "    while i!=iteration:\n",
        "      if i<len(currentStatesList):\n",
        "        if i<len(chunksList):\n",
        "          transcriptMap.append([currentStatesList[i], chunksList[i]])\n",
        "          i += 1\n",
        "          temp = currentStatesList[i]\n",
        "          continue\n",
        "      else:\n",
        "        transcriptMap.append([temp, chunksList[i]])\n",
        "        i += 1\n",
        "        continue\n",
        "      i += 1\n",
        "  return transcriptMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9tKffXJ7bkL"
      },
      "outputs": [],
      "source": [
        "dataframe = {\"Speaker\": [], \"Text\": []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aygXTfWpExGd"
      },
      "outputs": [],
      "source": [
        "from links_cluster import LinksCluster\n",
        "def SpeakerDiarizationAndSTT(audioLength):\n",
        "  duration = 20\n",
        "  count = 10\n",
        "  ByteData = bytearray()\n",
        "  fullFloatData = []\n",
        "  flag = 0\n",
        "  currentStatesStartIndex = 0\n",
        "  print(\"Recording Started...\")\n",
        "  while(count!=0):\n",
        "    ByteData, data = record(duration)\n",
        "    x,_ = librosa.load('audioLive.wav')\n",
        "    sf.write('audioLive_.wav', x, 20500)\n",
        "    \n",
        "    FloatData = byte_to_float(bytes(ByteData))\n",
        "    fullFloatData.extend(FloatData)\n",
        "\n",
        "    if flag==0:\n",
        "    #   embedding_obj = Embeddings('audioLive_.wav', 0, np.array(fullFloatData))\n",
        "      links_cluster = LinksCluster(0.95, 0.7, 1.0, store_vectors=True)\n",
        "      flag = 1\n",
        "    # else:\n",
        "    #   embedding_obj.update(np.array(FloatData))\n",
        "\n",
        "    embedding_obj = Embeddings('audioLive_.wav', 1)\n",
        "\n",
        "    currentStatesList = SpeakerDiarization(embedding_obj, links_cluster)\n",
        "    transcriptObtained, chunksList = get_large_audio_transcription('audioLive_.wav')\n",
        "    currentStatesStartIndex = len(currentStatesList)\n",
        "    mappedTranscript = getMappedTranscript(currentStatesList, chunksList)\n",
        "\n",
        "    # Adding the obtained transcript text and the speakers identities to the Dataframe\n",
        "    print(\"Writing Data...\")\n",
        "    for it in range(len(mappedTranscript)):\n",
        "      dataframe['Speaker'].append(mappedTranscript[it][0])\n",
        "      dataframe['Text'].append(mappedTranscript[it][1])\n",
        "\n",
        "    print(mappedTranscript)\n",
        "    # print(currentStatesList)\n",
        "    # print(chunksList)\n",
        "    count-=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fh8ekIJWTtrs",
        "outputId": "7e91fa13-52c0-4d74-d5b4-ce3f85bcbcf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recording Started...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "(70, 256)\n",
            "Writing Data...\n",
            "[[0, \"I don't want to watch a country deep space 9 is better. \"], [1, 'How big is deep space nine better than saturn 3. '], [2, 'Simple fraction.. '], [3, 'Please watch baby alive. '], [1, 'In what sense is that a compromise. '], [3, 'Fire. ']]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "(68, 256)\n",
            "Writing Data...\n",
            "[[1, 'Players familiar with each other will dive 75 to 80% of the time dude is limited number of alkanes. '], [3, 'Messages rock paper scissors lizard spock. '], [1, 'What. '], [1, \"It's really simple. \"], [1, 'Caesars. '], [1, 'Godspeed. '], [1, 'Femur. ']]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "(63, 256)\n",
            "Writing Data...\n",
            "[[3, 'Vaporizes draw. '], [1, 'As it always has draw precious caesars. '], [3, 'Okay i think i got it. '], [0, 'Paper scissors lizard spock. '], [1, 'Hello boys. '], [4, 'Ahoy matey. ']]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "(74, 256)\n",
            "Writing Data...\n",
            "[[1, 'Female peacock with trillion formation with enbrel engorged. '], [3, 'Deportes. '], [1, 'Or in this case the bar mitzvah boy with a pink eye. '], [1, 'Rock mutual but it works you show up at the club you show me the club in something distinctive. ']]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "(58, 256)\n",
            "Writing Data...\n",
            "[[3, \"I've got a whole list of them who wants to be my wingman. \"], [1, \"You're not going to need a wingman you're going to need a paramedic. \"], [3, 'U.s. quarters rocking my car. '], [3, 'Video game. ']]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "(46, 256)\n",
            "Writing Data...\n",
            "[[1, 'You are stupid scooter before i pick it up. '], [3, 'Android in the dumpster. '], [1, 'All right will you at least. ']]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "(60, 256)\n",
            "Writing Data...\n",
            "[[3, \"Oh thank god you're here. \"], [1, \"What's the emergency. \"], [3, 'I got the marshall constructing a ditch. '], [1, 'Where. '], [3, 'On a dusty highway just outside bakersfield what do you think on mars. '], [3, 'Is everything okay. '], [3, \"Ciao baby i'll be right here. \"]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "(74, 256)\n",
            "Writing Data...\n",
            "[[3, 'Incense work. '], [1, 'No other guys that i purchased it was a fiasco what did work was how do you like to visit a secret government facility. '], [3, 'So what exactly do you want us to do. '], [3, 'I need you and josh to get me some help to get stroller out of the ditch and i need you to get stephanie out before or no. ']]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "(74, 256)\n",
            "Writing Data...\n",
            "[[1, 'Government projects on distant planets. '], [3, 'Tim hortons. '], [6, 'You know. '], [1, \"It's getting late. \"], [3, \"So do i get to drive these things are what i'm sorry but something's come up. \"]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cpu in 0.01 seconds.\n",
            "(68, 256)\n",
            "Writing Data...\n",
            "[[1, 'Are you a scientist lahore. '], [3, 'No one scientist like howard. '], [1, 'My mother is going to love her. '], [7, 'Oh nice. '], [1, 'Maybe we can carpool when they visit you in federal prison. '], [1, \"I'm sorry i totally interrupted. \"]]\n"
          ]
        }
      ],
      "source": [
        "SpeakerDiarizationAndSTT(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zknFR9Jw_TBM",
        "outputId": "65fc8000-e4fe-4487-e9dd-f6f88336a97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Speaker                                               Text\n",
            "0          0                                       73 east on. \n",
            "1          1                             I don't want to work. \n",
            "2          0                                  Best 90s better. \n",
            "3          1      How is deep space nine better than saturn 3. \n",
            "4          2                                 It sticks better. \n",
            "..       ...                                                ...\n",
            "103        3                     No one scientist like howard. \n",
            "104        1                   My mother is going to love her. \n",
            "105        7                                          Oh nice. \n",
            "106        1  Maybe we can carpool when they visit you in fe...\n",
            "107        1                  I'm sorry i totally interrupted. \n",
            "\n",
            "[108 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "print(pd.DataFrame(dataframe))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dataframe)\n",
        "df.to_excel('data_1.xlsx')"
      ],
      "metadata": {
        "id": "_I7CpuhpZV0S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MegaProjectPart1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}